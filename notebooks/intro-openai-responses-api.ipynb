{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to OpenAI Responses API\n",
    "\n",
    "The Responses API is OpenAI's most advanced interface for generating model responses. It provides a powerful, stateful way to interact with models, supporting text and image inputs with text or JSON outputs. The API enables multi-turn conversations, tool integration (web search, file search, code interpreter, computer use), function calling for custom code execution, and advanced features like structured outputs and reasoning models. This notebook demonstrates all core operations including creating responses, managing conversation state, and understanding the response lifecycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the OpenAI library and initialize our client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Initialize the client\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a Model Response\n",
    "\n",
    "Creating a response generates model output from text, image, or file inputs. The API supports sophisticated features like tool integration, structured outputs, function calling, and conversation continuity via `previous_response_id` or `conversation` parameters. Responses can run synchronously or asynchronously (background mode) and support streaming for real-time output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Key Parameters\n\n- **`model`** (string, optional): Model ID like `\"gpt-5-mini\"` or `\"gpt-5\"`. Determines capabilities, performance, and cost.\n- **`input`** (string/array, optional): Text, images, or files to process. Can be a simple string or complex array of items.\n- **`instructions`** (string, optional): System/developer message for context. Not carried over when using `previous_response_id`.\n- **`temperature`** (number, 0-2, default 1): Controls randomness. Higher = more creative, lower = more deterministic.\n- **`max_output_tokens`** (integer, optional): Token limit for response including reasoning tokens.\n- **`tools`** (array, optional): Built-in tools (web search, file search) or custom functions the model can call.\n- **`tool_choice`** (string/object, optional): Controls tool selection (\"auto\", \"required\", \"none\", or specific tool).\n- **`text`** (object, optional): Configuration for text output format (plain text or structured JSON).\n- **`previous_response_id`** (string, optional): ID of previous response for multi-turn conversations.\n- **`conversation`** (string/object, optional): Conversation ID or object for persistent context.\n- **`metadata`** (map, optional): Custom key-value pairs (16 max, 64/512 char limits).\n- **`background`** (boolean, default false): Run asynchronously, enabling cancellation.\n- **`stream`** (boolean, default false): Stream response data in real-time via server-sent events.\n- **`store`** (boolean, default true): Whether to store response for later retrieval."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simple text response\nresponse = client.responses.create(\n    model=\"gpt-5-mini\",\n    input=\"Explain quantum entanglement in two sentences suitable for a high school student.\"\n)\n\nprint(f\"Response ID: {response.id}\")\nprint(f\"Model: {response.model}\")\nprint(f\"Status: {response.status}\")\nprint(f\"\\nOutput Text:\\n{response.output_text}\")\nprint(f\"\\nToken Usage:\")\nprint(f\"  Input: {response.usage.input_tokens}\")\nprint(f\"  Output: {response.usage.output_tokens}\")\nprint(f\"  Total: {response.usage.total_tokens}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Response with system instructions and temperature control\ncreative_response = client.responses.create(\n    model=\"gpt-5-mini\",\n    input=\"Write a haiku about artificial intelligence.\",\n    instructions=\"You are a creative poet who specializes in concise, evocative imagery.\",\n    temperature=1.2,\n    metadata={\"type\": \"poetry\", \"format\": \"haiku\"}\n)\n\nprint(f\"Creative Response ({creative_response.temperature} temperature):\\n\")\nprint(creative_response.output_text)\nprint(f\"\\nMetadata: {creative_response.metadata}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Multi-turn conversation using previous_response_id\nfirst_response = client.responses.create(\n    model=\"gpt-5-mini\",\n    input=\"What are the three laws of thermodynamics?\"\n)\n\nprint(\"First Response:\")\nprint(first_response.output_text)\nprint(f\"\\nResponse ID: {first_response.id}\")\n\n# Follow-up question using previous response\nsecond_response = client.responses.create(\n    model=\"gpt-5-mini\",\n    input=\"Can you give me a real-world example of the second law?\",\n    previous_response_id=first_response.id\n)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Follow-up Response:\")\nprint(second_response.output_text)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get a Model Response\n",
    "\n",
    "Retrieving a response allows you to access previously generated outputs by ID. This is essential for asynchronous workflows, conversation history retrieval, or inspecting stored responses. The endpoint supports streaming mode to fetch response data as it's being generated if the response is still in progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "- **`response_id`** (string, required): The unique ID of the response to retrieve.\n",
    "- **`include`** (array, optional): Additional data to include (web search sources, code outputs, image URLs, logprobs).\n",
    "- **`stream`** (boolean, optional): Stream the response data if still generating.\n",
    "- **`starting_after`** (integer, optional): Sequence number to start streaming from.\n",
    "- **`include_obfuscation`** (boolean, optional): Include random chars to normalize payload sizes (security feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a previously created response\n",
    "retrieved_response = client.responses.retrieve(response.id)\n",
    "\n",
    "print(f\"Retrieved Response ID: {retrieved_response.id}\")\n",
    "print(f\"Created At: {retrieved_response.created_at}\")\n",
    "print(f\"Status: {retrieved_response.status}\")\n",
    "print(f\"Model: {retrieved_response.model}\")\n",
    "print(f\"\\nOutput:\")\n",
    "print(retrieved_response.output_text)\n",
    "\n",
    "# Check if there was an error\n",
    "if retrieved_response.error:\n",
    "    print(f\"\\nError: {retrieved_response.error}\")\n",
    "else:\n",
    "    print(f\"\\nNo errors - response completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Delete a Model Response\n",
    "\n",
    "Deleting a response permanently removes it from storage. This is useful for managing data retention, removing sensitive information, or cleaning up test responses. The operation returns a deletion confirmation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "- **`response_id`** (string, required): The ID of the response to delete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a test response to delete\ntest_response = client.responses.create(\n    model=\"gpt-5-mini\",\n    input=\"This is a test response for deletion.\"\n)\n\nprint(f\"Created test response: {test_response.id}\")\n\n# Note: Uncomment to actually delete the response\n# deletion_result = client.responses.delete(test_response.id)\n# print(f\"\\nDeletion Result:\")\n# print(f\"  ID: {deletion_result.id}\")\n# print(f\"  Deleted: {deletion_result.deleted}\")\n\nprint(\"\\nSkipping actual deletion to preserve response for following examples\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cancel a Response\n",
    "\n",
    "Cancelling a response stops an in-progress generation. This operation only works for responses created with `background=true`. Cancellation is useful for managing long-running requests, handling user interruptions, or stopping unnecessary computation when requirements change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "- **`response_id`** (string, required): The ID of the background response to cancel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a background response that can be cancelled\nbackground_response = client.responses.create(\n    model=\"gpt-5-mini\",\n    input=\"Write a detailed essay about the history of computing, covering at least 10 major milestones.\",\n    background=True,\n    max_output_tokens=2000\n)\n\nprint(f\"Created background response: {background_response.id}\")\nprint(f\"Initial Status: {background_response.status}\")\n\n# Note: Uncomment to actually cancel the response\n# import time\n# time.sleep(1)  # Wait a moment\n# cancelled_response = client.responses.cancel(background_response.id)\n# print(f\"\\nCancelled Response Status: {cancelled_response.status}\")\n\nprint(\"\\nSkipping cancellation - response will complete normally\")\nprint(\"Note: Only responses with background=True can be cancelled\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. List Input Items\n",
    "\n",
    "Listing input items retrieves all content that was provided to the model when generating a specific response. This includes user messages, system instructions, images, files, and context from previous responses. The endpoint supports pagination and ordering, making it ideal for auditing, debugging, or understanding what context the model had when generating output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "- **`response_id`** (string, required): The response ID to list input items from.\n",
    "- **`limit`** (integer, optional): Number of items to return (1-100, default 20).\n",
    "- **`order`** (string, optional): Sort order - `\"asc\"` or `\"desc\"` (default).\n",
    "- **`after`** (string, optional): Item ID for pagination cursor.\n",
    "- **`include`** (array, optional): Additional data to include (images, logprobs, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List input items from a previous response\n",
    "input_items = client.responses.input_items.list(\n",
    "    response.id,\n",
    "    limit=10,\n",
    "    order=\"asc\"\n",
    ")\n",
    "\n",
    "print(f\"Input Items for Response {response.id}:\")\n",
    "print(f\"Total items retrieved: {len(input_items.data)}\")\n",
    "print(f\"Has more items: {input_items.has_more}\")\n",
    "print(f\"First item ID: {input_items.first_id}\")\n",
    "print(f\"Last item ID: {input_items.last_id}\")\n",
    "\n",
    "print(\"\\nItem Details:\")\n",
    "for i, item in enumerate(input_items.data, 1):\n",
    "    print(f\"\\n{i}. Item ID: {item.id}\")\n",
    "    print(f\"   Type: {item.type}\")\n",
    "    print(f\"   Role: {item.role}\")\n",
    "    if hasattr(item, 'content') and item.content:\n",
    "        for content_item in item.content:\n",
    "            if content_item.type == 'input_text':\n",
    "                text_preview = content_item.text[:80] + \"...\" if len(content_item.text) > 80 else content_item.text\n",
    "                print(f\"   Content: {text_preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List input items from a multi-turn conversation\n",
    "multi_turn_inputs = client.responses.input_items.list(\n",
    "    second_response.id,\n",
    "    limit=5\n",
    ")\n",
    "\n",
    "print(f\"Multi-turn Conversation Input Items:\")\n",
    "print(f\"Total items: {len(multi_turn_inputs.data)}\")\n",
    "print(\"\\nThis shows both the original question and the follow-up:\")\n",
    "for item in multi_turn_inputs.data:\n",
    "    if item.role == 'user' and item.content:\n",
    "        for content in item.content:\n",
    "            if hasattr(content, 'text'):\n",
    "                print(f\"  - {content.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Get Input Token Counts\n",
    "\n",
    "Token counting allows you to estimate costs and validate inputs before creating a response. This endpoint calculates the exact number of input tokens that would be used for a given set of parameters, including conversation history, instructions, tools, and formatting overhead. Essential for budgeting, rate limiting, and ensuring requests fit within context windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "- **`model`** (string, optional): Model ID to count tokens for (different models have different tokenization).\n",
    "- **`input`** (string/array, optional): The input content to count tokens for.\n",
    "- **`instructions`** (string, optional): System message to include in the count.\n",
    "- **`conversation`** (string/object, optional): Conversation context to include.\n",
    "- **`previous_response_id`** (string, optional): Previous response for multi-turn counting.\n",
    "- **`tools`** (array, optional): Tools configuration to include in count.\n",
    "- **`text`**, **`tool_choice`**, **`reasoning`** (various, optional): Other configuration affecting token count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Count tokens for a simple input\ntoken_count = client.responses.input_tokens.count(\n    model=\"gpt-5-mini\",\n    input=\"What is the capital of France?\"\n)\n\nprint(f\"Simple Query Token Count: {token_count.input_tokens}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Count tokens with instructions and longer input\ncomplex_token_count = client.responses.input_tokens.count(\n    model=\"gpt-5-mini\",\n    input=\"\"\"Analyze the following customer feedback and extract:\n    1. Sentiment (positive, negative, neutral)\n    2. Main topics mentioned\n    3. Action items for the product team\n    \n    Feedback: 'I love the new UI design, but the app crashes frequently when uploading large files. \n    The customer support team was very helpful though.'\"\"\",\n    instructions=\"You are an expert customer feedback analyst with experience in product management.\"\n)\n\nprint(f\"Complex Query Token Count: {complex_token_count.input_tokens}\")\nprint(f\"\\nThis includes:\")\nprint(f\"  - System instructions\")\nprint(f\"  - User input text\")\nprint(f\"  - Formatting overhead\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare token counts across different models\nmodels_to_compare = [\"gpt-5-mini\", \"gpt-5\"]\ntest_input = \"Explain the concept of machine learning in simple terms.\"\n\nprint(\"Token Count Comparison:\")\nfor model in models_to_compare:\n    count = client.responses.input_tokens.count(\n        model=model,\n        input=test_input\n    )\n    print(f\"  {model}: {count.input_tokens} tokens\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. The Response Object\n",
    "\n",
    "The Response object is the core data structure returned by the API, containing all information about a model generation including outputs, metadata, configuration, and usage statistics. Understanding this structure is crucial for extracting results, handling errors, and managing conversation state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Object Structure\n",
    "\n",
    "**Core Identification:**\n",
    "- **`id`** (string): Unique identifier (e.g., `\"resp_67ccd2bed1ec8190b14f964abc0542670bb6a6b452d3795b\"`).\n",
    "- **`object`** (string): Always `\"response\"` for this type.\n",
    "- **`created_at`** (number): Unix timestamp of creation.\n",
    "\n",
    "**Status and Completion:**\n",
    "- **`status`** (string): One of `completed`, `failed`, `in_progress`, `cancelled`, `queued`, or `incomplete`.\n",
    "- **`error`** (object): Error details if status is `failed`.\n",
    "- **`incomplete_details`** (object): Reason for incomplete status.\n",
    "\n",
    "**Output:**\n",
    "- **`output`** (array): Generated content items (messages, tool calls, reasoning).\n",
    "- **`output_text`** (string, SDK only): Aggregated text from all output items (convenience property).\n",
    "\n",
    "**Configuration:**\n",
    "- **`model`** (string): Model ID used.\n",
    "- **`instructions`** (string/array): System messages provided.\n",
    "- **`temperature`**, **`top_p`** (number): Sampling parameters.\n",
    "- **`max_output_tokens`** (integer): Token limit.\n",
    "- **`tools`** (array): Available tools.\n",
    "- **`tool_choice`** (string/object): Tool selection strategy.\n",
    "- **`text`** (object): Output format configuration.\n",
    "\n",
    "**Conversation State:**\n",
    "- **`conversation`** (object): Linked conversation if any.\n",
    "- **`previous_response_id`** (string): Previous response in chain.\n",
    "\n",
    "**Usage and Metadata:**\n",
    "- **`usage`** (object): Token counts (input, output, cached, reasoning).\n",
    "- **`metadata`** (map): Custom key-value pairs.\n",
    "- **`store`** (boolean): Whether response is stored.\n",
    "- **`background`** (boolean): Whether run in background mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Examine a complete response object\nsample_response = client.responses.create(\n    model=\"gpt-5-mini\",\n    input=\"Describe photosynthesis in one paragraph.\",\n    instructions=\"You are a biology teacher explaining concepts to high school students.\",\n    temperature=0.7,\n    max_output_tokens=200,\n    metadata={\"subject\": \"biology\", \"topic\": \"photosynthesis\"}\n)\n\nprint(\"Response Object Structure:\\n\")\nprint(f\"ID: {sample_response.id}\")\nprint(f\"Object Type: {sample_response.object}\")\nprint(f\"Created At: {sample_response.created_at}\")\nprint(f\"Status: {sample_response.status}\")\nprint(f\"\\nModel Configuration:\")\nprint(f\"  Model: {sample_response.model}\")\nprint(f\"  Temperature: {sample_response.temperature}\")\nprint(f\"  Max Output Tokens: {sample_response.max_output_tokens}\")\nprint(f\"  Top P: {sample_response.top_p}\")\nprint(f\"\\nOutput:\")\nprint(f\"  Number of output items: {len(sample_response.output)}\")\nprint(f\"  Output text: {sample_response.output_text[:100]}...\")\nprint(f\"\\nUsage Statistics:\")\nprint(f\"  Input Tokens: {sample_response.usage.input_tokens}\")\nprint(f\"  Output Tokens: {sample_response.usage.output_tokens}\")\nprint(f\"  Total Tokens: {sample_response.usage.total_tokens}\")\nprint(f\"  Cached Tokens: {sample_response.usage.input_tokens_details.cached_tokens}\")\nprint(f\"\\nMetadata: {sample_response.metadata}\")\nprint(f\"Store: {sample_response.store}\")\nprint(f\"Background: {sample_response.background}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the output array structure\n",
    "print(\"Output Array Structure:\\n\")\n",
    "for i, output_item in enumerate(sample_response.output, 1):\n",
    "    print(f\"Output Item {i}:\")\n",
    "    print(f\"  Type: {output_item.type}\")\n",
    "    print(f\"  ID: {output_item.id}\")\n",
    "    print(f\"  Status: {output_item.status}\")\n",
    "    print(f\"  Role: {output_item.role}\")\n",
    "    \n",
    "    if hasattr(output_item, 'content'):\n",
    "        print(f\"  Content Items: {len(output_item.content)}\")\n",
    "        for j, content in enumerate(output_item.content, 1):\n",
    "            print(f\"    {j}. Type: {content.type}\")\n",
    "            if hasattr(content, 'text'):\n",
    "                print(f\"       Text length: {len(content.text)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. The Input Item List\n",
    "\n",
    "The Input Item List represents all content provided to the model when generating a response. This includes user messages, system instructions, conversation history from previous responses, and any additional context. Understanding this structure helps with debugging, auditing input costs, and managing conversation state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Item List Structure\n",
    "\n",
    "- **`object`** (string): Always `\"list\"` for list responses.\n",
    "- **`data`** (array): Array of input items used to generate the response.\n",
    "  - Each item can be a message, tool result, or other input type\n",
    "  - Contains `id`, `type`, `role`, and `content` fields\n",
    "- **`first_id`** (string): ID of the first item in the current page.\n",
    "- **`last_id`** (string): ID of the last item in the current page.\n",
    "- **`has_more`** (boolean): Whether additional items exist beyond this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine input item list structure\n",
    "input_list = client.responses.input_items.list(\n",
    "    sample_response.id,\n",
    "    limit=10\n",
    ")\n",
    "\n",
    "print(\"Input Item List Structure:\\n\")\n",
    "print(f\"Object Type: {input_list.object}\")\n",
    "print(f\"Number of Items: {len(input_list.data)}\")\n",
    "print(f\"First Item ID: {input_list.first_id}\")\n",
    "print(f\"Last Item ID: {input_list.last_id}\")\n",
    "print(f\"Has More Items: {input_list.has_more}\")\n",
    "\n",
    "print(\"\\nItem Breakdown:\")\n",
    "for i, item in enumerate(input_list.data, 1):\n",
    "    print(f\"\\nItem {i}:\")\n",
    "    print(f\"  ID: {item.id}\")\n",
    "    print(f\"  Type: {item.type}\")\n",
    "    print(f\"  Role: {item.role}\")\n",
    "    \n",
    "    if hasattr(item, 'content') and item.content:\n",
    "        print(f\"  Content Items: {len(item.content)}\")\n",
    "        for content_item in item.content:\n",
    "            print(f\"    - Type: {content_item.type}\")\n",
    "            if hasattr(content_item, 'text'):\n",
    "                preview = content_item.text[:60] + \"...\" if len(content_item.text) > 60 else content_item.text\n",
    "                print(f\"      Text: {preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate pagination with input items\n# First, create a response with multiple input items via conversation\nconversation = client.conversations.create(\n    items=[\n        {\"type\": \"message\", \"role\": \"user\", \"content\": \"What is Python?\"},\n        {\"type\": \"message\", \"role\": \"user\", \"content\": \"What are its main features?\"},\n        {\"type\": \"message\", \"role\": \"user\", \"content\": \"Give me a code example.\"}\n    ]\n)\n\nconv_response = client.responses.create(\n    model=\"gpt-5-mini\",\n    conversation=conversation.id,\n    input=\"Now explain list comprehensions.\"\n)\n\n# List with pagination\nfirst_page = client.responses.input_items.list(\n    conv_response.id,\n    limit=2,\n    order=\"asc\"\n)\n\nprint(f\"First Page of Input Items:\")\nprint(f\"  Items in this page: {len(first_page.data)}\")\nprint(f\"  Has more: {first_page.has_more}\")\n\nif first_page.has_more:\n    second_page = client.responses.input_items.list(\n        conv_response.id,\n        limit=2,\n        after=first_page.last_id\n    )\n    print(f\"\\nSecond Page of Input Items:\")\n    print(f\"  Items in this page: {len(second_page.data)}\")\n    print(f\"  Has more: {second_page.has_more}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered the complete OpenAI Responses API including:\n",
    "\n",
    "1. **Creating responses** - Text generation with instructions, temperature control, and multi-turn conversations\n",
    "2. **Retrieving responses** - Accessing stored responses by ID\n",
    "3. **Deleting responses** - Removing responses from storage\n",
    "4. **Cancelling responses** - Stopping background response generation\n",
    "5. **Listing input items** - Inspecting what was sent to the model\n",
    "6. **Counting tokens** - Estimating costs and validating inputs\n",
    "7. **Response object structure** - Understanding output format and metadata\n",
    "8. **Input item list structure** - Managing conversation context and pagination\n",
    "\n",
    "The Responses API provides the most advanced interface for working with OpenAI models, enabling sophisticated applications with tool integration, structured outputs, conversation state management, and fine-grained control over model behavior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}