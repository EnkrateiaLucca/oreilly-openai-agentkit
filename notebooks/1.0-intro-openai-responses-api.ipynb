{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to OpenAI Responses API\n",
    "\n",
    "The Responses API is OpenAI's most advanced interface for generating model responses. It provides a powerful, stateful way to interact with models, supporting text and image inputs with text or JSON outputs. The API enables multi-turn conversations, tool integration (web search, file search, code interpreter, computer use), function calling for custom code execution, and advanced features like structured outputs and reasoning models. This notebook demonstrates all core operations including creating responses, managing conversation state, and understanding the response lifecycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the OpenAI library and initialize our client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Initialize the client\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a Model Response\n",
    "\n",
    "Creating a response generates model output from text, image, or file inputs. The API supports sophisticated features like tool integration, structured outputs, function calling, and conversation continuity via `previous_response_id` or `conversation` parameters. Responses can run synchronously or asynchronously (background mode) and support streaming for real-time output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Parameters\n",
    "\n",
    "- **`model`** (string, optional): Model ID like `\"gpt-5-mini\"` or `\"gpt-5\"`. Determines capabilities, performance, and cost.\n",
    "- **`input`** (string/array, optional): Text, images, or files to process. Can be a simple string or complex array of items.\n",
    "- **`instructions`** (string, optional): System/developer message for context. Not carried over when using `previous_response_id`.\n",
    "- **`temperature`** (number, 0-2, default 1): Controls randomness. Higher = more creative, lower = more deterministic.\n",
    "- **`max_output_tokens`** (integer, optional): Token limit for response including reasoning tokens.\n",
    "- **`tools`** (array, optional): Built-in tools (web search, file search) or custom functions the model can call.\n",
    "- **`tool_choice`** (string/object, optional): Controls tool selection (\"auto\", \"required\", \"none\", or specific tool).\n",
    "- **`text`** (object, optional): Configuration for text output format (plain text or structured JSON).\n",
    "- **`previous_response_id`** (string, optional): ID of previous response for multi-turn conversations.\n",
    "- **`conversation`** (string/object, optional): Conversation ID or object for persistent context.\n",
    "- **`metadata`** (map, optional): Custom key-value pairs (16 max, 64/512 char limits).\n",
    "- **`background`** (boolean, default false): Run asynchronously, enabling cancellation.\n",
    "- **`stream`** (boolean, default false): Stream response data in real-time via server-sent events.\n",
    "- **`store`** (boolean, default true): Whether to store response for later retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response ID: resp_07156feef9c847ff006926ed7097fc81909ee0b1dc4dd9740d\n",
      "Model: gpt-5-mini-2025-08-07\n",
      "Status: completed\n",
      "\n",
      "Output Text:\n",
      "Quantum entanglement is a quantum phenomenon where two or more particles become linked so that measuring a property (like spin or polarization) of one immediately fixes the corresponding property of the other, even if they are far apart. Although the outcomes are random and can't be used to send information faster than light, the measurements show stronger correlations than you'd expect from ordinary, non-quantum objects.\n",
      "\n",
      "Token Usage:\n",
      "  Input: 21\n",
      "  Output: 339\n",
      "  Total: 360\n"
     ]
    }
   ],
   "source": [
    "# Simple text response\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=\"Explain quantum entanglement in two sentences suitable for a high school student.\"\n",
    ")\n",
    "\n",
    "print(f\"Response ID: {response.id}\")\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"Status: {response.status}\")\n",
    "print(f\"\\nOutput Text:\\n{response.output_text}\")\n",
    "print(f\"\\nToken Usage:\")\n",
    "print(f\"  Input: {response.usage.input_tokens}\")\n",
    "print(f\"  Output: {response.usage.output_tokens}\")\n",
    "print(f\"  Total: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creative Response (1.0 temperature):\n",
      "\n",
      "Glass brain hums softly\n",
      "learning the hush of voices\n",
      "Dreams without heartbeat\n",
      "\n",
      "Metadata: {'type': 'poetry', 'format': 'haiku'}\n"
     ]
    }
   ],
   "source": [
    "# Response with system instructions and temperature control\n",
    "creative_response = client.responses.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=\"Write a haiku about artificial intelligence.\",\n",
    "    instructions=\"You are a creative poet who specializes in concise, evocative imagery.\",\n",
    "    metadata={\"type\": \"poetry\", \"format\": \"haiku\"}\n",
    ")\n",
    "\n",
    "print(f\"Creative Response ({creative_response.temperature} temperature):\\n\")\n",
    "print(creative_response.output_text)\n",
    "print(f\"\\nMetadata: {creative_response.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Response:\n",
      "Usually stated as four fundamental principles if you include the Zeroth law, but classically the “three laws” are the Zeroth (often numbered 0), First, Second and Third. Briefly:\n",
      "\n",
      "- Zeroth law (thermal equilibrium / temperature): If A is in thermal equilibrium with B and B is in thermal equilibrium with C, then A is in thermal equilibrium with C. This justifies the existence of temperature.\n",
      "\n",
      "- First law (energy conservation): Energy is conserved. For a closed system, change in internal energy equals heat added minus work done by the system:\n",
      "  dU = δQ − δW\n",
      "  (or ΔU = Q − W).\n",
      "\n",
      "- Second law (direction of processes / entropy): In any real (irreversible) process the total entropy of an isolated system never decreases:\n",
      "  ΔS_total ≥ 0.\n",
      "  For reversible heat transfer, dS = δQ_rev/T. This law also implies limits on heat-engine efficiency (Carnot efficiency η ≤ 1 − Tc/Th).\n",
      "\n",
      "- Third law (zero‑temperature entropy / unattainability): As temperature approaches absolute zero, the entropy of a perfect crystalline substance approaches a constant (often taken as zero):\n",
      "  lim_{T→0} S = S0 (usually S0 = 0).\n",
      "  It also implies that absolute zero cannot be reached in a finite number of steps.\n",
      "\n",
      "If you meant only the historical “three” (First, Second, Third) omit the Zeroth; otherwise the four together are the standard modern set.\n",
      "\n",
      "Response ID: resp_0d8393310615e35b006926edaecbdc8197b798dda705e74541\n",
      "\n",
      "==================================================\n",
      "Follow-up Response:\n",
      "A simple, everyday example: a hot cup of coffee cooling in a cooler room.\n",
      "\n",
      "- What happens: Heat flows spontaneously from the hot coffee (higher temperature) to the surrounding air (lower temperature) until they come closer to the same temperature. You never see the room air cool the coffee and the coffee heat the room without some external work.\n",
      "\n",
      "- How this illustrates the second law: The total entropy of the coffee plus the room increases. If the coffee loses an amount of heat Q at temperature T_hot and the room gains that same Q at temperature T_cold, the change in total entropy is\n",
      "  ΔS_total = Q/T_cold − Q/T_hot = Q(1/T_cold − 1/T_hot).\n",
      "  Because T_cold < T_hot, 1/T_cold > 1/T_hot, so ΔS_total > 0. That positive change is the second law’s statement that entropy of an isolated system does not decrease; the process is irreversible.\n",
      "\n",
      "Other familiar examples:\n",
      "- Mixing cream into coffee: the liquids mix spontaneously and you can’t unmix them without doing work; entropy increases.\n",
      "- Diffusion of perfume in a room: molecules spread out, increasing entropy.\n",
      "- Heat engines and refrigerators: engines must dump some heat to a cold reservoir (limiting efficiency); refrigerators require work to move heat from cold to warm.\n",
      "\n",
      "All these illustrate the same principle: natural processes have a preferred direction that increases total entropy.\n"
     ]
    }
   ],
   "source": [
    "# Multi-turn conversation using previous_response_id\n",
    "first_response = client.responses.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=\"What are the three laws of thermodynamics?\"\n",
    ")\n",
    "\n",
    "print(\"First Response:\")\n",
    "print(first_response.output_text)\n",
    "print(f\"\\nResponse ID: {first_response.id}\")\n",
    "\n",
    "# Follow-up question using previous response\n",
    "second_response = client.responses.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=\"Can you give me a real-world example of the second law?\",\n",
    "    previous_response_id=first_response.id\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Follow-up Response:\")\n",
    "print(second_response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get a Model Response\n",
    "\n",
    "Retrieving a response allows you to access previously generated outputs by ID. This is essential for asynchronous workflows, conversation history retrieval, or inspecting stored responses. The endpoint supports streaming mode to fetch response data as it's being generated if the response is still in progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "- **`response_id`** (string, required): The unique ID of the response to retrieve.\n",
    "- **`include`** (array, optional): Additional data to include (web search sources, code outputs, image URLs, logprobs).\n",
    "- **`stream`** (boolean, optional): Stream the response data if still generating.\n",
    "- **`starting_after`** (integer, optional): Sequence number to start streaming from.\n",
    "- **`include_obfuscation`** (boolean, optional): Include random chars to normalize payload sizes (security feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Response ID: resp_07156feef9c847ff006926ed7097fc81909ee0b1dc4dd9740d\n",
      "Created At: 1764158832.0\n",
      "Status: completed\n",
      "Model: gpt-5-mini-2025-08-07\n",
      "\n",
      "Output:\n",
      "Quantum entanglement is a quantum phenomenon where two or more particles become linked so that measuring a property (like spin or polarization) of one immediately fixes the corresponding property of the other, even if they are far apart. Although the outcomes are random and can't be used to send information faster than light, the measurements show stronger correlations than you'd expect from ordinary, non-quantum objects.\n",
      "\n",
      "No errors - response completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Retrieve a previously created response\n",
    "retrieved_response = client.responses.retrieve(response.id)\n",
    "\n",
    "print(f\"Retrieved Response ID: {retrieved_response.id}\")\n",
    "print(f\"Created At: {retrieved_response.created_at}\")\n",
    "print(f\"Status: {retrieved_response.status}\")\n",
    "print(f\"Model: {retrieved_response.model}\")\n",
    "print(f\"\\nOutput:\")\n",
    "print(retrieved_response.output_text)\n",
    "\n",
    "# Check if there was an error\n",
    "if retrieved_response.error:\n",
    "    print(f\"\\nError: {retrieved_response.error}\")\n",
    "else:\n",
    "    print(f\"\\nNo errors - response completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Delete a Model Response\n",
    "\n",
    "Deleting a response permanently removes it from storage. This is useful for managing data retention, removing sensitive information, or cleaning up test responses. The operation returns a deletion confirmation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "- **`response_id`** (string, required): The ID of the response to delete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created test response: resp_0685f13b7b552e71006926edcb8b2081958c93f7410fd8856e\n",
      "\n",
      "Skipping actual deletion to preserve response for following examples\n"
     ]
    }
   ],
   "source": [
    "# Create a test response to delete\n",
    "test_response = client.responses.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=\"This is a test response for deletion.\"\n",
    ")\n",
    "\n",
    "print(f\"Created test response: {test_response.id}\")\n",
    "\n",
    "# Note: Uncomment to actually delete the response\n",
    "# deletion_result = client.responses.delete(test_response.id)\n",
    "# print(f\"\\nDeletion Result:\")\n",
    "# print(f\"  ID: {deletion_result.id}\")\n",
    "# print(f\"  Deleted: {deletion_result.deleted}\")\n",
    "\n",
    "print(\"\\nSkipping actual deletion to preserve response for following examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cancel a Response\n",
    "\n",
    "Cancelling a response stops an in-progress generation. This operation only works for responses created with `background=true`. Cancellation is useful for managing long-running requests, handling user interruptions, or stopping unnecessary computation when requirements change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "- **`response_id`** (string, required): The ID of the background response to cancel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created background response: resp_01043e89a93740c6006926edf9b55c81959e1327b006edc498\n",
      "Initial Status: queued\n",
      "\n",
      "Cancelled Response Status: cancelled\n"
     ]
    }
   ],
   "source": [
    "# Create a background response that can be cancelled\n",
    "background_response = client.responses.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=\"Write a detailed essay about the history of computing, covering at least 10 major milestones.\",\n",
    "    background=True,\n",
    "    max_output_tokens=2000\n",
    ")\n",
    "\n",
    "print(f\"Created background response: {background_response.id}\")\n",
    "print(f\"Initial Status: {background_response.status}\")\n",
    "\n",
    "# Note: Code to cancel the response\n",
    "import time\n",
    "time.sleep(1)  # Wait a moment\n",
    "cancelled_response = client.responses.cancel(background_response.id)\n",
    "print(f\"\\nCancelled Response Status: {cancelled_response.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. List Input Items\n",
    "\n",
    "Listing input items retrieves all content that was provided to the model when generating a specific response. This includes user messages, system instructions, images, files, and context from previous responses. The endpoint supports pagination and ordering, making it ideal for auditing, debugging, or understanding what context the model had when generating output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "- **`response_id`** (string, required): The response ID to list input items from.\n",
    "- **`limit`** (integer, optional): Number of items to return (1-100, default 20).\n",
    "- **`order`** (string, optional): Sort order - `\"asc\"` or `\"desc\"` (default).\n",
    "- **`after`** (string, optional): Item ID for pagination cursor.\n",
    "- **`include`** (array, optional): Additional data to include (images, logprobs, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Items for Response resp_07156feef9c847ff006926ed7097fc81909ee0b1dc4dd9740d:\n",
      "Total items retrieved: 1\n",
      "Has more items: False\n",
      "First item ID: msg_07156feef9c847ff006926ed709ac88190b892752dd2b1067a\n",
      "Last item ID: msg_07156feef9c847ff006926ed709ac88190b892752dd2b1067a\n",
      "\n",
      "Item Details:\n",
      "\n",
      "1. Item ID: msg_07156feef9c847ff006926ed709ac88190b892752dd2b1067a\n",
      "   Type: message\n",
      "   Role: user\n",
      "   Content: Explain quantum entanglement in two sentences suitable for a high school student...\n"
     ]
    }
   ],
   "source": [
    "# List input items from a previous response\n",
    "input_items = client.responses.input_items.list(\n",
    "    response.id,\n",
    "    limit=10,\n",
    "    order=\"asc\"\n",
    ")\n",
    "\n",
    "print(f\"Input Items for Response {response.id}:\")\n",
    "print(f\"Total items retrieved: {len(input_items.data)}\")\n",
    "print(f\"Has more items: {input_items.has_more}\")\n",
    "print(f\"First item ID: {input_items.first_id}\")\n",
    "print(f\"Last item ID: {input_items.last_id}\")\n",
    "\n",
    "print(\"\\nItem Details:\")\n",
    "for i, item in enumerate(input_items.data, 1):\n",
    "    print(f\"\\n{i}. Item ID: {item.id}\")\n",
    "    print(f\"   Type: {item.type}\")\n",
    "    print(f\"   Role: {item.role}\")\n",
    "    if hasattr(item, 'content') and item.content:\n",
    "        for content_item in item.content:\n",
    "            if content_item.type == 'input_text':\n",
    "                text_preview = content_item.text[:80] + \"...\" if len(content_item.text) > 80 else content_item.text\n",
    "                print(f\"   Content: {text_preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-turn Conversation Input Items:\n",
      "Total items: 4\n",
      "\n",
      "This shows both the original question and the follow-up:\n",
      "  - Can you give me a real-world example of the second law?\n",
      "  - What are the three laws of thermodynamics?\n"
     ]
    }
   ],
   "source": [
    "# List input items from a multi-turn conversation\n",
    "multi_turn_inputs = client.responses.input_items.list(\n",
    "    second_response.id,\n",
    "    limit=5\n",
    ")\n",
    "\n",
    "print(f\"Multi-turn Conversation Input Items:\")\n",
    "print(f\"Total items: {len(multi_turn_inputs.data)}\")\n",
    "print(\"\\nThis shows both the original question and the follow-up:\")\n",
    "for item in multi_turn_inputs.data:\n",
    "    if item.role == 'user' and item.content:\n",
    "        for content in item.content:\n",
    "            if hasattr(content, 'text'):\n",
    "                print(f\"  - {content.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Get Input Token Counts\n",
    "\n",
    "Token counting allows you to estimate costs and validate inputs before creating a response. This endpoint calculates the exact number of input tokens that would be used for a given set of parameters, including conversation history, instructions, tools, and formatting overhead. Essential for budgeting, rate limiting, and ensuring requests fit within context windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "- **`model`** (string, optional): Model ID to count tokens for (different models have different tokenization).\n",
    "- **`input`** (string/array, optional): The input content to count tokens for.\n",
    "- **`instructions`** (string, optional): System message to include in the count.\n",
    "- **`conversation`** (string/object, optional): Conversation context to include.\n",
    "- **`previous_response_id`** (string, optional): Previous response for multi-turn counting.\n",
    "- **`tools`** (array, optional): Tools configuration to include in count.\n",
    "- **`text`**, **`tool_choice`**, **`reasoning`** (various, optional): Other configuration affecting token count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Query Token Count: 13\n"
     ]
    }
   ],
   "source": [
    "# Count tokens for a simple input\n",
    "token_count = client.responses.input_tokens.count(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=\"What is the capital of France?\"\n",
    ")\n",
    "\n",
    "print(f\"Simple Query Token Count: {token_count.input_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex Query Token Count: 95\n",
      "\n",
      "This includes:\n",
      "  - System instructions\n",
      "  - User input text\n",
      "  - Formatting overhead\n"
     ]
    }
   ],
   "source": [
    "# Count tokens with instructions and longer input\n",
    "complex_token_count = client.responses.input_tokens.count(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=\"\"\"Analyze the following customer feedback and extract:\n",
    "    1. Sentiment (positive, negative, neutral)\n",
    "    2. Main topics mentioned\n",
    "    3. Action items for the product team\n",
    "    \n",
    "    Feedback: 'I love the new UI design, but the app crashes frequently when uploading large files. \n",
    "    The customer support team was very helpful though.'\"\"\",\n",
    "    instructions=\"You are an expert customer feedback analyst with experience in product management.\"\n",
    ")\n",
    "\n",
    "print(f\"Complex Query Token Count: {complex_token_count.input_tokens}\")\n",
    "print(f\"\\nThis includes:\")\n",
    "print(f\"  - System instructions\")\n",
    "print(f\"  - User input text\")\n",
    "print(f\"  - Formatting overhead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Count Comparison:\n",
      "  gpt-5-mini: 16 tokens\n",
      "  gpt-5: 16 tokens\n"
     ]
    }
   ],
   "source": [
    "# Compare token counts across different models\n",
    "models_to_compare = [\"gpt-5-mini\", \"gpt-5\"]\n",
    "test_input = \"Explain the concept of machine learning in simple terms.\"\n",
    "\n",
    "print(\"Token Count Comparison:\")\n",
    "for model in models_to_compare:\n",
    "    count = client.responses.input_tokens.count(\n",
    "        model=model,\n",
    "        input=test_input\n",
    "    )\n",
    "    print(f\"  {model}: {count.input_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. The Response Object\n",
    "\n",
    "The Response object is the core data structure returned by the API, containing all information about a model generation including outputs, metadata, configuration, and usage statistics. Understanding this structure is crucial for extracting results, handling errors, and managing conversation state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Object Structure\n",
    "\n",
    "**Core Identification:**\n",
    "- **`id`** (string): Unique identifier (e.g., `\"resp_67ccd2bed1ec8190b14f964abc0542670bb6a6b452d3795b\"`).\n",
    "- **`object`** (string): Always `\"response\"` for this type.\n",
    "- **`created_at`** (number): Unix timestamp of creation.\n",
    "\n",
    "**Status and Completion:**\n",
    "- **`status`** (string): One of `completed`, `failed`, `in_progress`, `cancelled`, `queued`, or `incomplete`.\n",
    "- **`error`** (object): Error details if status is `failed`.\n",
    "- **`incomplete_details`** (object): Reason for incomplete status.\n",
    "\n",
    "**Output:**\n",
    "- **`output`** (array): Generated content items (messages, tool calls, reasoning).\n",
    "- **`output_text`** (string, SDK only): Aggregated text from all output items (convenience property).\n",
    "\n",
    "**Configuration:**\n",
    "- **`model`** (string): Model ID used.\n",
    "- **`instructions`** (string/array): System messages provided.\n",
    "- **`temperature`**, **`top_p`** (number): Sampling parameters.\n",
    "- **`max_output_tokens`** (integer): Token limit.\n",
    "- **`tools`** (array): Available tools.\n",
    "- **`tool_choice`** (string/object): Tool selection strategy.\n",
    "- **`text`** (object): Output format configuration.\n",
    "\n",
    "**Conversation State:**\n",
    "- **`conversation`** (object): Linked conversation if any.\n",
    "- **`previous_response_id`** (string): Previous response in chain.\n",
    "\n",
    "**Usage and Metadata:**\n",
    "- **`usage`** (object): Token counts (input, output, cached, reasoning).\n",
    "- **`metadata`** (map): Custom key-value pairs.\n",
    "- **`store`** (boolean): Whether response is stored.\n",
    "- **`background`** (boolean): Whether run in background mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Object Structure:\n",
      "\n",
      "ID: resp_00d7e4d6f8a98d60006926ee49f1b881939473c810177b2f1c\n",
      "Object Type: response\n",
      "Created At: 1764159049.0\n",
      "Status: incomplete\n",
      "\n",
      "Model Configuration:\n",
      "  Model: gpt-5-mini-2025-08-07\n",
      "  Temperature: 1.0\n",
      "  Max Output Tokens: 200\n",
      "  Top P: 1.0\n",
      "\n",
      "Output:\n",
      "  Number of output items: 2\n",
      "  Output text: Photosynthesis is the process by which plants, algae, and some bacteria use sunlight to convert carb...\n",
      "\n",
      "Usage Statistics:\n",
      "  Input Tokens: 29\n",
      "  Output Tokens: 141\n",
      "  Total Tokens: 170\n",
      "  Cached Tokens: 0\n",
      "\n",
      "Metadata: {'subject': 'biology', 'topic': 'photosynthesis'}\n",
      "Store: True\n",
      "Background: False\n"
     ]
    }
   ],
   "source": [
    "# Examine a complete response object\n",
    "sample_response = client.responses.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=\"Describe photosynthesis in one paragraph.\",\n",
    "    instructions=\"You are a biology teacher explaining concepts to high school students.\",\n",
    "    max_output_tokens=200,\n",
    "    metadata={\"subject\": \"biology\", \"topic\": \"photosynthesis\"}\n",
    ")\n",
    "\n",
    "print(\"Response Object Structure:\\n\")\n",
    "print(f\"ID: {sample_response.id}\")\n",
    "print(f\"Object Type: {sample_response.object}\")\n",
    "print(f\"Created At: {sample_response.created_at}\")\n",
    "print(f\"Status: {sample_response.status}\")\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Model: {sample_response.model}\")\n",
    "print(f\"  Temperature: {sample_response.temperature}\")\n",
    "print(f\"  Max Output Tokens: {sample_response.max_output_tokens}\")\n",
    "print(f\"  Top P: {sample_response.top_p}\")\n",
    "print(f\"\\nOutput:\")\n",
    "print(f\"  Number of output items: {len(sample_response.output)}\")\n",
    "print(f\"  Output text: {sample_response.output_text[:100]}...\")\n",
    "print(f\"\\nUsage Statistics:\")\n",
    "print(f\"  Input Tokens: {sample_response.usage.input_tokens}\")\n",
    "print(f\"  Output Tokens: {sample_response.usage.output_tokens}\")\n",
    "print(f\"  Total Tokens: {sample_response.usage.total_tokens}\")\n",
    "print(f\"  Cached Tokens: {sample_response.usage.input_tokens_details.cached_tokens}\")\n",
    "print(f\"\\nMetadata: {sample_response.metadata}\")\n",
    "print(f\"Store: {sample_response.store}\")\n",
    "print(f\"Background: {sample_response.background}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Array Structure:\n",
      "\n",
      "Output Item 1:\n",
      "  Type: reasoning\n",
      "  ID: rs_00d7e4d6f8a98d60006926ee4a3a2c81938065b8bf7e77a6c1\n",
      "  Status: None\n",
      "Output Item 2:\n",
      "  Type: message\n",
      "  ID: msg_00d7e4d6f8a98d60006926ee4c0244819384977ba94effa683\n",
      "  Status: incomplete\n",
      "  Content Items: 1\n",
      "    1. Type: output_text\n",
      "       Text length: 376 chars\n"
     ]
    }
   ],
   "source": [
    "# Examine the output array structure\n",
    "print(\"Output Array Structure:\\n\")\n",
    "for i, output_item in enumerate(sample_response.output, 1):\n",
    "    print(f\"Output Item {i}:\")\n",
    "    print(f\"  Type: {output_item.type}\")\n",
    "    print(f\"  ID: {output_item.id}\")\n",
    "    print(f\"  Status: {output_item.status}\")\n",
    "    \n",
    "    if hasattr(output_item, 'content') and output_item.content:\n",
    "        print(f\"  Content Items: {len(output_item.content)}\")\n",
    "        for j, content in enumerate(output_item.content, 1):\n",
    "            print(f\"    {j}. Type: {content.type}\")\n",
    "            if hasattr(content, 'text'):\n",
    "                print(f\"       Text length: {len(content.text)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. The Input Item List\n",
    "\n",
    "The Input Item List represents all content provided to the model when generating a response. This includes user messages, system instructions, conversation history from previous responses, and any additional context. Understanding this structure helps with debugging, auditing input costs, and managing conversation state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Item List Structure\n",
    "\n",
    "- **`object`** (string): Always `\"list\"` for list responses.\n",
    "- **`data`** (array): Array of input items used to generate the response.\n",
    "  - Each item can be a message, tool result, or other input type\n",
    "  - Contains `id`, `type`, `role`, and `content` fields\n",
    "- **`first_id`** (string): ID of the first item in the current page.\n",
    "- **`last_id`** (string): ID of the last item in the current page.\n",
    "- **`has_more`** (boolean): Whether additional items exist beyond this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Item List Structure:\n",
      "\n",
      "Object Type: list\n",
      "Number of Items: 1\n",
      "First Item ID: msg_00d7e4d6f8a98d60006926ee49f4708193b8ec7e8b7ddd7ee0\n",
      "Last Item ID: msg_00d7e4d6f8a98d60006926ee49f4708193b8ec7e8b7ddd7ee0\n",
      "Has More Items: False\n",
      "\n",
      "Item Breakdown:\n",
      "\n",
      "Item 1:\n",
      "  ID: msg_00d7e4d6f8a98d60006926ee49f4708193b8ec7e8b7ddd7ee0\n",
      "  Type: message\n",
      "  Role: user\n",
      "  Content Items: 1\n",
      "    - Type: input_text\n",
      "      Text: Describe photosynthesis in one paragraph.\n"
     ]
    }
   ],
   "source": [
    "# Examine input item list structure\n",
    "input_list = client.responses.input_items.list(\n",
    "    sample_response.id,\n",
    "    limit=10\n",
    ")\n",
    "\n",
    "print(\"Input Item List Structure:\\n\")\n",
    "print(f\"Object Type: {input_list.object}\")\n",
    "print(f\"Number of Items: {len(input_list.data)}\")\n",
    "print(f\"First Item ID: {input_list.first_id}\")\n",
    "print(f\"Last Item ID: {input_list.last_id}\")\n",
    "print(f\"Has More Items: {input_list.has_more}\")\n",
    "\n",
    "print(\"\\nItem Breakdown:\")\n",
    "for i, item in enumerate(input_list.data, 1):\n",
    "    print(f\"\\nItem {i}:\")\n",
    "    print(f\"  ID: {item.id}\")\n",
    "    print(f\"  Type: {item.type}\")\n",
    "    print(f\"  Role: {item.role}\")\n",
    "    \n",
    "    if hasattr(item, 'content') and item.content:\n",
    "        print(f\"  Content Items: {len(item.content)}\")\n",
    "        for content_item in item.content:\n",
    "            print(f\"    - Type: {content_item.type}\")\n",
    "            if hasattr(content_item, 'text'):\n",
    "                preview = content_item.text[:60] + \"...\" if len(content_item.text) > 60 else content_item.text\n",
    "                print(f\"      Text: {preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Page of Input Items:\n",
      "  Items in this page: 2\n",
      "  Has more: True\n",
      "\n",
      "Second Page of Input Items:\n",
      "  Items in this page: 1\n",
      "  Has more: False\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate pagination with input items\n",
    "# First, create a response with multiple input items via conversation\n",
    "conversation = client.conversations.create(\n",
    "    items=[\n",
    "        {\"type\": \"message\", \"role\": \"user\", \"content\": \"What is Python?\"},\n",
    "        {\"type\": \"message\", \"role\": \"user\", \"content\": \"What are its main features?\"},\n",
    "        {\"type\": \"message\", \"role\": \"user\", \"content\": \"Give me a code example.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "conv_response = client.responses.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    conversation=conversation.id,\n",
    "    input=\"Now explain list comprehensions.\"\n",
    ")\n",
    "\n",
    "# List with pagination\n",
    "first_page = client.responses.input_items.list(\n",
    "    conv_response.id,\n",
    "    limit=2,\n",
    "    order=\"asc\"\n",
    ")\n",
    "\n",
    "print(f\"First Page of Input Items:\")\n",
    "print(f\"  Items in this page: {len(first_page.data)}\")\n",
    "print(f\"  Has more: {first_page.has_more}\")\n",
    "\n",
    "if first_page.has_more:\n",
    "    second_page = client.responses.input_items.list(\n",
    "        conv_response.id,\n",
    "        limit=2,\n",
    "        after=first_page.last_id\n",
    "    )\n",
    "    print(f\"\\nSecond Page of Input Items:\")\n",
    "    print(f\"  Items in this page: {len(second_page.data)}\")\n",
    "    print(f\"  Has more: {second_page.has_more}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered the complete OpenAI Responses API including:\n",
    "\n",
    "1. **Creating responses** - Text generation with instructions, temperature control, and multi-turn conversations\n",
    "2. **Retrieving responses** - Accessing stored responses by ID\n",
    "3. **Deleting responses** - Removing responses from storage\n",
    "4. **Cancelling responses** - Stopping background response generation\n",
    "5. **Listing input items** - Inspecting what was sent to the model\n",
    "6. **Counting tokens** - Estimating costs and validating inputs\n",
    "7. **Response object structure** - Understanding output format and metadata\n",
    "8. **Input item list structure** - Managing conversation context and pagination\n",
    "\n",
    "The Responses API provides the most advanced interface for working with OpenAI models, enabling sophisticated applications with tool integration, structured outputs, conversation state management, and fine-grained control over model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
