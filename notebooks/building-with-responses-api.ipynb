{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Agentic Applications with the Responses API\n",
    "\n",
    "This notebook presents practical, reusable patterns and functions for building production-ready agentic applications using OpenAI's Responses API. Each pattern addresses common challenges in developing chat agents and agentic workflows, with complete working examples you can adapt for your own projects.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- **Streaming responses** with real-time callbacks\n",
    "- **Retry logic** with exponential backoff for reliability\n",
    "- **Token budget management** to control costs\n",
    "- **Multi-turn conversations** with context management\n",
    "- **Error handling** patterns for production systems\n",
    "- **Function calling** for tool integration\n",
    "- **Structured outputs** with validation\n",
    "- **Cost tracking** for budget monitoring\n",
    "- **Context window management** for long conversations\n",
    "- **Response caching** for efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import Optional, Callable, Dict, List, Any\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize the client\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Streaming Response Handler\n",
    "\n",
    "When building interactive agents, users expect real-time feedback. This pattern shows how to stream responses with callbacks for different events (content, completion, errors). This is essential for chat interfaces where you want to display text as it's generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_response(\n",
    "    client: OpenAI,\n",
    "    model: str,\n",
    "    input_text: str,\n",
    "    on_content: Optional[Callable[[str], None]] = None,\n",
    "    on_complete: Optional[Callable[[Dict], None]] = None,\n",
    "    on_error: Optional[Callable[[Exception], None]] = None,\n",
    "    **kwargs\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Stream a response with callbacks for real-time updates.\n",
    "    \n",
    "    Args:\n",
    "        client: OpenAI client instance\n",
    "        model: Model to use\n",
    "        input_text: Input prompt\n",
    "        on_content: Callback for each content chunk (receives text)\n",
    "        on_complete: Callback when stream completes (receives final response)\n",
    "        on_error: Callback for errors (receives exception)\n",
    "        **kwargs: Additional parameters for responses.create()\n",
    "    \n",
    "    Returns:\n",
    "        Complete response text\n",
    "    \"\"\"\n",
    "    full_text = \"\"\n",
    "    \n",
    "    try:\n",
    "        stream = client.responses.create(\n",
    "            model=model,\n",
    "            input=input_text,\n",
    "            stream=True,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        for chunk in stream:\n",
    "            # Extract text from the chunk\n",
    "            if hasattr(chunk, 'output') and chunk.output:\n",
    "                for output_item in chunk.output:\n",
    "                    if hasattr(output_item, 'content'):\n",
    "                        for content_item in output_item.content:\n",
    "                            if hasattr(content_item, 'text'):\n",
    "                                text = content_item.text\n",
    "                                full_text += text\n",
    "                                if on_content:\n",
    "                                    on_content(text)\n",
    "        \n",
    "        if on_complete:\n",
    "            on_complete({\"text\": full_text, \"status\": \"completed\"})\n",
    "        \n",
    "        return full_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        if on_error:\n",
    "            on_error(e)\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "def print_chunk(text: str):\n",
    "    print(text, end='', flush=True)\n",
    "\n",
    "def on_done(response: Dict):\n",
    "    print(f\"\\n\\nâœ“ Complete! Total length: {len(response['text'])} chars\")\n",
    "\n",
    "print(\"Agent: \", end='')\n",
    "response_text = stream_response(\n",
    "    client,\n",
    "    model=\"gpt-5-mini\",\n",
    "    input_text=\"Explain how neural networks learn in 3 sentences.\",\n",
    "    on_content=print_chunk,\n",
    "    on_complete=on_done\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retry Logic with Exponential Backoff\n",
    "\n",
    "Production systems need to handle transient failures gracefully. This pattern implements exponential backoff for retrying failed requests - essential for building reliable agents that can recover from temporary API issues, rate limits, or network problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import APIError, RateLimitError, APITimeoutError\n",
    "\n",
    "def create_response_with_retry(\n",
    "    client: OpenAI,\n",
    "    max_retries: int = 3,\n",
    "    initial_delay: float = 1.0,\n",
    "    exponential_base: float = 2.0,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a response with automatic retry logic.\n",
    "    \n",
    "    Args:\n",
    "        client: OpenAI client\n",
    "        max_retries: Maximum number of retry attempts\n",
    "        initial_delay: Initial delay in seconds\n",
    "        exponential_base: Multiplier for exponential backoff\n",
    "        **kwargs: Parameters for responses.create()\n",
    "    \n",
    "    Returns:\n",
    "        Response object\n",
    "    \"\"\"\n",
    "    retryable_errors = (RateLimitError, APITimeoutError, APIError)\n",
    "    \n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            response = client.responses.create(**kwargs)\n",
    "            return response\n",
    "        \n",
    "        except retryable_errors as e:\n",
    "            if attempt == max_retries:\n",
    "                print(f\"âŒ Max retries ({max_retries}) exceeded\")\n",
    "                raise\n",
    "            \n",
    "            delay = initial_delay * (exponential_base ** attempt)\n",
    "            print(f\"âš ï¸  Attempt {attempt + 1} failed: {type(e).__name__}\")\n",
    "            print(f\"   Retrying in {delay:.1f}s...\")\n",
    "            time.sleep(delay)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Non-retryable error\n",
    "            print(f\"âŒ Non-retryable error: {type(e).__name__}\")\n",
    "            raise\n",
    "\n",
    "# Example usage\n",
    "response = create_response_with_retry(\n",
    "    client,\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=\"What is machine learning?\",\n",
    "    max_retries=3\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Success! Response: {response.output_text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Token Budget Manager\n",
    "\n",
    "Managing costs is critical in production. This pattern helps you enforce token budgets, track usage across multiple requests, and prevent unexpected costs - essential for any agent that makes multiple API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TokenBudget:\n",
    "    \"\"\"Track token usage against a budget.\"\"\"\n",
    "    max_tokens: int\n",
    "    used_tokens: int = 0\n",
    "    \n",
    "    def can_spend(self, tokens: int) -> bool:\n",
    "        \"\"\"Check if we can spend the specified tokens.\"\"\"\n",
    "        return self.used_tokens + tokens <= self.max_tokens\n",
    "    \n",
    "    def spend(self, tokens: int) -> None:\n",
    "        \"\"\"Record token usage.\"\"\"\n",
    "        self.used_tokens += tokens\n",
    "    \n",
    "    def remaining(self) -> int:\n",
    "        \"\"\"Get remaining token budget.\"\"\"\n",
    "        return self.max_tokens - self.used_tokens\n",
    "    \n",
    "    def usage_percentage(self) -> float:\n",
    "        \"\"\"Get budget usage as percentage.\"\"\"\n",
    "        return (self.used_tokens / self.max_tokens) * 100\n",
    "\n",
    "class BudgetedClient:\n",
    "    \"\"\"OpenAI client wrapper with token budget enforcement.\"\"\"\n",
    "    \n",
    "    def __init__(self, client: OpenAI, budget: TokenBudget):\n",
    "        self.client = client\n",
    "        self.budget = budget\n",
    "    \n",
    "    def create_response(self, **kwargs):\n",
    "        \"\"\"Create response while enforcing budget.\"\"\"\n",
    "        # Estimate tokens for this request\n",
    "        estimated = self._estimate_tokens(kwargs)\n",
    "        \n",
    "        if not self.budget.can_spend(estimated):\n",
    "            raise ValueError(\n",
    "                f\"Token budget exceeded. Remaining: {self.budget.remaining()}, \"\n",
    "                f\"Needed: {estimated}\"\n",
    "            )\n",
    "        \n",
    "        # Create response\n",
    "        response = self.client.responses.create(**kwargs)\n",
    "        \n",
    "        # Track actual usage\n",
    "        actual_tokens = response.usage.total_tokens\n",
    "        self.budget.spend(actual_tokens)\n",
    "        \n",
    "        print(f\"ðŸ“Š Tokens used: {actual_tokens} | \"\n",
    "              f\"Remaining: {self.budget.remaining()} | \"\n",
    "              f\"Budget: {self.budget.usage_percentage():.1f}% used\")\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _estimate_tokens(self, kwargs: Dict) -> int:\n",
    "        \"\"\"Estimate tokens for a request.\"\"\"\n",
    "        # Use the count endpoint for accurate estimation\n",
    "        count_result = self.client.responses.input_tokens.count(**kwargs)\n",
    "        \n",
    "        # Add estimated output tokens (or use max_output_tokens if specified)\n",
    "        estimated_output = kwargs.get('max_output_tokens', 500)\n",
    "        \n",
    "        return count_result.input_tokens + estimated_output\n",
    "\n",
    "# Example usage\n",
    "budget = TokenBudget(max_tokens=5000)\n",
    "budgeted_client = BudgetedClient(client, budget)\n",
    "\n",
    "# Make multiple requests within budget\n",
    "for i in range(3):\n",
    "    try:\n",
    "        response = budgeted_client.create_response(\n",
    "            model=\"gpt-5-mini\",\n",
    "            input=f\"What is interesting fact #{i+1} about space?\",\n",
    "            max_output_tokens=100\n",
    "        )\n",
    "        print(f\"Response {i+1}: {response.output_text[:80]}...\\n\")\n",
    "    except ValueError as e:\n",
    "        print(f\"âŒ Budget limit reached: {e}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Turn Conversation Builder\n",
    "\n",
    "Most agents need to maintain context across multiple turns. This pattern provides a clean interface for building conversations, managing history, and handling context windows - the foundation of any chat agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationBuilder:\n",
    "    \"\"\"Build and manage multi-turn conversations.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        client: OpenAI,\n",
    "        model: str,\n",
    "        instructions: Optional[str] = None,\n",
    "        max_history: int = 10\n",
    "    ):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.instructions = instructions\n",
    "        self.max_history = max_history\n",
    "        self.history: List[Dict[str, str]] = []\n",
    "        self.last_response_id: Optional[str] = None\n",
    "    \n",
    "    def send(self, user_input: str, **kwargs) -> str:\n",
    "        \"\"\"Send a message and get response.\"\"\"\n",
    "        # Create response with history\n",
    "        response = self.client.responses.create(\n",
    "            model=self.model,\n",
    "            input=user_input,\n",
    "            instructions=self.instructions if not self.last_response_id else None,\n",
    "            previous_response_id=self.last_response_id,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # Update history\n",
    "        self.history.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_input,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        self.history.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": response.output_text,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"tokens\": response.usage.total_tokens\n",
    "        })\n",
    "        \n",
    "        # Trim history if needed\n",
    "        if len(self.history) > self.max_history * 2:  # * 2 for user + assistant\n",
    "            self.history = self.history[-(self.max_history * 2):]\n",
    "        \n",
    "        self.last_response_id = response.id\n",
    "        \n",
    "        return response.output_text\n",
    "    \n",
    "    def get_history(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Get conversation history.\"\"\"\n",
    "        return self.history\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset conversation.\"\"\"\n",
    "        self.history = []\n",
    "        self.last_response_id = None\n",
    "    \n",
    "    def format_history(self) -> str:\n",
    "        \"\"\"Format history as readable text.\"\"\"\n",
    "        formatted = []\n",
    "        for msg in self.history:\n",
    "            role = \"User\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
    "            formatted.append(f\"{role}: {msg['content']}\")\n",
    "        return \"\\n\\n\".join(formatted)\n",
    "\n",
    "# Example usage: Building a coding tutor agent\n",
    "tutor = ConversationBuilder(\n",
    "    client,\n",
    "    model=\"gpt-5-mini\",\n",
    "    instructions=\"You are a patient coding tutor. Explain concepts clearly and provide examples.\",\n",
    "    max_history=5\n",
    ")\n",
    "\n",
    "# Simulate a conversation\n",
    "questions = [\n",
    "    \"What is a variable in Python?\",\n",
    "    \"Can you show me an example?\",\n",
    "    \"What's the difference between a list and a tuple?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\nðŸ§‘ User: {question}\")\n",
    "    response = tutor.send(question)\n",
    "    print(f\"ðŸ¤– Tutor: {response[:150]}...\")\n",
    "\n",
    "# Show conversation summary\n",
    "print(f\"\\nðŸ“Š Conversation stats:\")\n",
    "print(f\"   Total turns: {len(tutor.history) // 2}\")\n",
    "print(f\"   Total tokens: {sum(msg.get('tokens', 0) for msg in tutor.history)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Handling Wrapper\n",
    "\n",
    "Robust error handling is essential for production agents. This pattern provides a comprehensive wrapper that handles different error types, provides meaningful feedback, and ensures graceful degradation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import Union\n",
    "\n",
    "class ErrorSeverity(Enum):\n",
    "    \"\"\"Error severity levels.\"\"\"\n",
    "    LOW = \"low\"\n",
    "    MEDIUM = \"medium\"\n",
    "    HIGH = \"high\"\n",
    "    CRITICAL = \"critical\"\n",
    "\n",
    "@dataclass\n",
    "class ErrorResult:\n",
    "    \"\"\"Structured error result.\"\"\"\n",
    "    error_type: str\n",
    "    message: str\n",
    "    severity: ErrorSeverity\n",
    "    retryable: bool\n",
    "    suggestion: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class SuccessResult:\n",
    "    \"\"\"Successful response result.\"\"\"\n",
    "    response: Any\n",
    "    text: str\n",
    "    tokens: int\n",
    "\n",
    "Result = Union[SuccessResult, ErrorResult]\n",
    "\n",
    "def safe_create_response(\n",
    "    client: OpenAI,\n",
    "    **kwargs\n",
    ") -> Result:\n",
    "    \"\"\"\n",
    "    Create response with comprehensive error handling.\n",
    "    \n",
    "    Returns:\n",
    "        Either SuccessResult or ErrorResult\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.responses.create(**kwargs)\n",
    "        \n",
    "        # Check for failed status\n",
    "        if response.status == \"failed\":\n",
    "            return ErrorResult(\n",
    "                error_type=\"ResponseFailed\",\n",
    "                message=f\"Response failed: {response.error}\",\n",
    "                severity=ErrorSeverity.HIGH,\n",
    "                retryable=True,\n",
    "                suggestion=\"Check input parameters and try again\"\n",
    "            )\n",
    "        \n",
    "        return SuccessResult(\n",
    "            response=response,\n",
    "            text=response.output_text,\n",
    "            tokens=response.usage.total_tokens\n",
    "        )\n",
    "    \n",
    "    except RateLimitError as e:\n",
    "        return ErrorResult(\n",
    "            error_type=\"RateLimit\",\n",
    "            message=\"Rate limit exceeded\",\n",
    "            severity=ErrorSeverity.MEDIUM,\n",
    "            retryable=True,\n",
    "            suggestion=\"Wait a moment and retry with exponential backoff\"\n",
    "        )\n",
    "    \n",
    "    except APITimeoutError as e:\n",
    "        return ErrorResult(\n",
    "            error_type=\"Timeout\",\n",
    "            message=\"Request timed out\",\n",
    "            severity=ErrorSeverity.MEDIUM,\n",
    "            retryable=True,\n",
    "            suggestion=\"Retry the request or reduce complexity\"\n",
    "        )\n",
    "    \n",
    "    except APIError as e:\n",
    "        return ErrorResult(\n",
    "            error_type=\"APIError\",\n",
    "            message=str(e),\n",
    "            severity=ErrorSeverity.HIGH,\n",
    "            retryable=True,\n",
    "            suggestion=\"Check API status and retry\"\n",
    "        )\n",
    "    \n",
    "    except ValueError as e:\n",
    "        return ErrorResult(\n",
    "            error_type=\"ValueError\",\n",
    "            message=str(e),\n",
    "            severity=ErrorSeverity.HIGH,\n",
    "            retryable=False,\n",
    "            suggestion=\"Check input parameters\"\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        return ErrorResult(\n",
    "            error_type=\"Unknown\",\n",
    "            message=str(e),\n",
    "            severity=ErrorSeverity.CRITICAL,\n",
    "            retryable=False,\n",
    "            suggestion=\"Review error details and contact support if needed\"\n",
    "        )\n",
    "\n",
    "# Example usage\n",
    "result = safe_create_response(\n",
    "    client,\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=\"Explain quantum computing briefly.\"\n",
    ")\n",
    "\n",
    "if isinstance(result, SuccessResult):\n",
    "    print(f\"âœ… Success!\")\n",
    "    print(f\"Response: {result.text[:100]}...\")\n",
    "    print(f\"Tokens: {result.tokens}\")\n",
    "else:\n",
    "    print(f\"âŒ Error: {result.error_type}\")\n",
    "    print(f\"Message: {result.message}\")\n",
    "    print(f\"Severity: {result.severity.value}\")\n",
    "    print(f\"Retryable: {result.retryable}\")\n",
    "    if result.suggestion:\n",
    "        print(f\"ðŸ’¡ Suggestion: {result.suggestion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Function Calling Pattern\n",
    "\n",
    "Function calling enables agents to interact with external systems and tools. This pattern shows how to define tools, handle function calls, and execute them - essential for building agents that can take actions.\n",
    "\n",
    "**Note:** The calculator example uses a simplified approach for demonstration. In production, use a safe expression evaluator library like `simpleeval` or `asteval` instead of Python's `eval()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolRegistry:\n",
    "    \"\"\"Registry for managing available tools/functions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tools: Dict[str, Callable] = {}\n",
    "        self.tool_definitions: List[Dict] = []\n",
    "    \n",
    "    def register(self, name: str, description: str, parameters: Dict):\n",
    "        \"\"\"Decorator to register a tool.\"\"\"\n",
    "        def decorator(func: Callable):\n",
    "            self.tools[name] = func\n",
    "            self.tool_definitions.append({\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": name,\n",
    "                    \"description\": description,\n",
    "                    \"parameters\": parameters\n",
    "                }\n",
    "            })\n",
    "            return func\n",
    "        return decorator\n",
    "    \n",
    "    def execute(self, name: str, arguments: Dict) -> Any:\n",
    "        \"\"\"Execute a registered tool.\"\"\"\n",
    "        if name not in self.tools:\n",
    "            raise ValueError(f\"Tool '{name}' not found\")\n",
    "        return self.tools[name](**arguments)\n",
    "    \n",
    "    def get_definitions(self) -> List[Dict]:\n",
    "        \"\"\"Get tool definitions for API.\"\"\"\n",
    "        return self.tool_definitions\n",
    "\n",
    "# Create registry and register tools\n",
    "registry = ToolRegistry()\n",
    "\n",
    "@registry.register(\n",
    "    name=\"get_weather\",\n",
    "    description=\"Get current weather for a location\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"City name or location\"\n",
    "            },\n",
    "            \"units\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                \"description\": \"Temperature units\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"location\"]\n",
    "    }\n",
    ")\n",
    "def get_weather(location: str, units: str = \"celsius\") -> str:\n",
    "    \"\"\"Simulate weather API call.\"\"\"\n",
    "    # In production, this would call a real weather API\n",
    "    return json.dumps({\n",
    "        \"location\": location,\n",
    "        \"temperature\": 22,\n",
    "        \"units\": units,\n",
    "        \"condition\": \"Partly cloudy\"\n",
    "    })\n",
    "\n",
    "@registry.register(\n",
    "    name=\"calculate\",\n",
    "    description=\"Perform simple mathematical calculations (addition, subtraction, multiplication, division)\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"operation\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"],\n",
    "                \"description\": \"Mathematical operation to perform\"\n",
    "            },\n",
    "            \"a\": {\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"First number\"\n",
    "            },\n",
    "            \"b\": {\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"Second number\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"operation\", \"a\", \"b\"]\n",
    "    }\n",
    ")\n",
    "def calculate(operation: str, a: float, b: float) -> str:\n",
    "    \"\"\"Safely perform basic mathematical operations.\"\"\"\n",
    "    try:\n",
    "        operations = {\n",
    "            \"add\": lambda x, y: x + y,\n",
    "            \"subtract\": lambda x, y: x - y,\n",
    "            \"multiply\": lambda x, y: x * y,\n",
    "            \"divide\": lambda x, y: x / y if y != 0 else \"Error: Division by zero\"\n",
    "        }\n",
    "        \n",
    "        result = operations[operation](a, b)\n",
    "        return json.dumps({\"result\": result, \"operation\": operation, \"a\": a, \"b\": b})\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e)})\n",
    "\n",
    "def run_agent_with_tools(\n",
    "    client: OpenAI,\n",
    "    registry: ToolRegistry,\n",
    "    user_input: str,\n",
    "    model: str = \"gpt-5-mini\",\n",
    "    max_iterations: int = 5\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Run agent with tool calling capability.\n",
    "    \n",
    "    Handles the full tool calling loop:\n",
    "    1. Send user input\n",
    "    2. Check for tool calls\n",
    "    3. Execute tools\n",
    "    4. Send results back\n",
    "    5. Get final response\n",
    "    \"\"\"\n",
    "    previous_response_id = None\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"\\nðŸ”„ Iteration {iteration + 1}\")\n",
    "        \n",
    "        # Create response with tools\n",
    "        response = client.responses.create(\n",
    "            model=model,\n",
    "            input=user_input if iteration == 0 else None,\n",
    "            tools=registry.get_definitions(),\n",
    "            previous_response_id=previous_response_id\n",
    "        )\n",
    "        \n",
    "        previous_response_id = response.id\n",
    "        \n",
    "        # Check for tool calls in output\n",
    "        tool_calls = []\n",
    "        for output_item in response.output:\n",
    "            if output_item.type == \"function_call\":\n",
    "                tool_calls.append(output_item)\n",
    "        \n",
    "        if not tool_calls:\n",
    "            # No tool calls, we have the final response\n",
    "            print(\"âœ… Final response received\")\n",
    "            return response.output_text\n",
    "        \n",
    "        # Execute tool calls\n",
    "        print(f\"ðŸ”§ Executing {len(tool_calls)} tool(s)...\")\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            arguments = json.loads(tool_call.function.arguments)\n",
    "            \n",
    "            print(f\"   Calling: {function_name}({arguments})\")\n",
    "            result = registry.execute(function_name, arguments)\n",
    "            print(f\"   Result: {result[:80]}...\")\n",
    "            \n",
    "            # Send tool result back to the model\n",
    "            # Note: In the actual Responses API, you would send the tool results\n",
    "            # as input in the next iteration\n",
    "    \n",
    "    return \"Max iterations reached\"\n",
    "\n",
    "# Example usage\n",
    "result = run_agent_with_tools(\n",
    "    client,\n",
    "    registry,\n",
    "    user_input=\"What's the weather in San Francisco? Also calculate 15 multiplied by 24.\"\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“ Final Response:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Structured Output Parser\n",
    "\n",
    "When building agents that need to return data in specific formats (JSON, lists, etc.), structured outputs ensure reliability. This pattern shows how to enforce and validate structured responses - crucial for agents that integrate with other systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type, TypeVar\n",
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "T = TypeVar('T', bound=BaseModel)\n",
    "\n",
    "def create_structured_response(\n",
    "    client: OpenAI,\n",
    "    model: str,\n",
    "    input_text: str,\n",
    "    schema: Type[T],\n",
    "    instructions: Optional[str] = None,\n",
    "    **kwargs\n",
    ") -> T:\n",
    "    \"\"\"\n",
    "    Create response with structured output and validation.\n",
    "    \n",
    "    Args:\n",
    "        client: OpenAI client\n",
    "        model: Model to use\n",
    "        input_text: User input\n",
    "        schema: Pydantic model class for validation\n",
    "        instructions: Optional instructions\n",
    "        **kwargs: Additional parameters\n",
    "    \n",
    "    Returns:\n",
    "        Validated instance of schema\n",
    "    \"\"\"\n",
    "    # Convert Pydantic model to JSON schema\n",
    "    json_schema = schema.model_json_schema()\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        input=input_text,\n",
    "        instructions=instructions,\n",
    "        text={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": schema.__name__,\n",
    "                \"schema\": json_schema,\n",
    "                \"strict\": True\n",
    "            }\n",
    "        },\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    # Parse and validate response\n",
    "    try:\n",
    "        # Extract JSON from output\n",
    "        json_text = response.output_text\n",
    "        data = json.loads(json_text)\n",
    "        \n",
    "        # Validate with Pydantic\n",
    "        validated = schema(**data)\n",
    "        return validated\n",
    "    \n",
    "    except (json.JSONDecodeError, ValidationError) as e:\n",
    "        raise ValueError(f\"Failed to parse structured output: {e}\")\n",
    "\n",
    "# Define schemas for different use cases\n",
    "class SentimentAnalysis(BaseModel):\n",
    "    \"\"\"Structured output for sentiment analysis.\"\"\"\n",
    "    text: str\n",
    "    sentiment: str  # positive, negative, neutral\n",
    "    confidence: float\n",
    "    key_phrases: List[str]\n",
    "\n",
    "class TaskExtraction(BaseModel):\n",
    "    \"\"\"Extract tasks from text.\"\"\"\n",
    "    task: str\n",
    "    priority: str  # high, medium, low\n",
    "    due_date: Optional[str] = None\n",
    "    assignee: Optional[str] = None\n",
    "\n",
    "class TaskList(BaseModel):\n",
    "    \"\"\"List of extracted tasks.\"\"\"\n",
    "    tasks: List[TaskExtraction]\n",
    "    total_count: int\n",
    "\n",
    "# Example 1: Sentiment Analysis\n",
    "print(\"Example 1: Sentiment Analysis\")\n",
    "sentiment = create_structured_response(\n",
    "    client,\n",
    "    model=\"gpt-5-mini\",\n",
    "    input_text=\"This product is absolutely amazing! Best purchase I've made all year.\",\n",
    "    schema=SentimentAnalysis,\n",
    "    instructions=\"Analyze the sentiment of the given text.\"\n",
    ")\n",
    "\n",
    "print(f\"Sentiment: {sentiment.sentiment}\")\n",
    "print(f\"Confidence: {sentiment.confidence}\")\n",
    "print(f\"Key phrases: {', '.join(sentiment.key_phrases)}\")\n",
    "\n",
    "# Example 2: Task Extraction\n",
    "print(\"\\nExample 2: Task Extraction\")\n",
    "email_text = \"\"\"\n",
    "Hi team,\n",
    "We need to complete the user auth by Friday. \n",
    "Also, someone should review the API docs (low priority).\n",
    "John, can you deploy the staging environment ASAP?\n",
    "\"\"\"\n",
    "\n",
    "tasks = create_structured_response(\n",
    "    client,\n",
    "    model=\"gpt-5-mini\",\n",
    "    input_text=email_text,\n",
    "    schema=TaskList,\n",
    "    instructions=\"Extract all tasks from the email with priorities and assignees.\"\n",
    ")\n",
    "\n",
    "print(f\"Found {tasks.total_count} tasks:\")\n",
    "for task in tasks.tasks:\n",
    "    print(f\"  [{task.priority}] {task.task}\")\n",
    "    if task.assignee:\n",
    "        print(f\"    Assignee: {task.assignee}\")\n",
    "    if task.due_date:\n",
    "        print(f\"    Due: {task.due_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cost Tracking Utility\n",
    "\n",
    "Understanding costs is essential for production agents. This pattern tracks token usage, estimates costs based on model pricing, and provides reports - critical for managing budgets in production systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelPricing:\n",
    "    \"\"\"Pricing per million tokens.\"\"\"\n",
    "    input_price: float  # per million input tokens\n",
    "    output_price: float  # per million output tokens\n",
    "    cached_input_price: float = 0.0  # per million cached tokens\n",
    "\n",
    "# Current pricing (update as needed)\n",
    "PRICING = {\n",
    "    \"gpt-5-mini\": ModelPricing(input_price=0.10, output_price=0.40),\n",
    "    \"gpt-5\": ModelPricing(input_price=1.00, output_price=4.00),\n",
    "}\n",
    "\n",
    "class CostTracker:\n",
    "    \"\"\"Track costs across multiple API calls.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.calls: List[Dict] = []\n",
    "        self.total_input_tokens = 0\n",
    "        self.total_output_tokens = 0\n",
    "        self.total_cost = 0.0\n",
    "    \n",
    "    def track_response(\n",
    "        self,\n",
    "        response,\n",
    "        model: str,\n",
    "        metadata: Optional[Dict] = None\n",
    "    ) -> Dict:\n",
    "        \"\"\"Track a response and calculate cost.\"\"\"\n",
    "        usage = response.usage\n",
    "        pricing = PRICING.get(model)\n",
    "        \n",
    "        if not pricing:\n",
    "            raise ValueError(f\"No pricing data for model: {model}\")\n",
    "        \n",
    "        # Calculate costs\n",
    "        input_cost = (usage.input_tokens / 1_000_000) * pricing.input_price\n",
    "        output_cost = (usage.output_tokens / 1_000_000) * pricing.output_price\n",
    "        total_cost = input_cost + output_cost\n",
    "        \n",
    "        # Handle cached tokens if available\n",
    "        cached_cost = 0.0\n",
    "        if hasattr(usage, 'input_tokens_details'):\n",
    "            cached = usage.input_tokens_details.cached_tokens\n",
    "            if cached > 0:\n",
    "                cached_cost = (cached / 1_000_000) * pricing.cached_input_price\n",
    "        \n",
    "        call_data = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model\": model,\n",
    "            \"response_id\": response.id,\n",
    "            \"input_tokens\": usage.input_tokens,\n",
    "            \"output_tokens\": usage.output_tokens,\n",
    "            \"total_tokens\": usage.total_tokens,\n",
    "            \"input_cost\": input_cost,\n",
    "            \"output_cost\": output_cost,\n",
    "            \"cached_cost\": cached_cost,\n",
    "            \"total_cost\": total_cost,\n",
    "            \"metadata\": metadata or {}\n",
    "        }\n",
    "        \n",
    "        self.calls.append(call_data)\n",
    "        self.total_input_tokens += usage.input_tokens\n",
    "        self.total_output_tokens += usage.output_tokens\n",
    "        self.total_cost += total_cost\n",
    "        \n",
    "        return call_data\n",
    "    \n",
    "    def get_summary(self) -> Dict:\n",
    "        \"\"\"Get cost summary.\"\"\"\n",
    "        return {\n",
    "            \"total_calls\": len(self.calls),\n",
    "            \"total_input_tokens\": self.total_input_tokens,\n",
    "            \"total_output_tokens\": self.total_output_tokens,\n",
    "            \"total_tokens\": self.total_input_tokens + self.total_output_tokens,\n",
    "            \"total_cost\": self.total_cost,\n",
    "            \"avg_cost_per_call\": self.total_cost / len(self.calls) if self.calls else 0,\n",
    "        }\n",
    "    \n",
    "    def print_report(self):\n",
    "        \"\"\"Print detailed cost report.\"\"\"\n",
    "        summary = self.get_summary()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"COST REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total API Calls: {summary['total_calls']}\")\n",
    "        print(f\"Total Tokens: {summary['total_tokens']:,}\")\n",
    "        print(f\"  Input: {summary['total_input_tokens']:,}\")\n",
    "        print(f\"  Output: {summary['total_output_tokens']:,}\")\n",
    "        print(f\"\\nTotal Cost: ${summary['total_cost']:.4f}\")\n",
    "        print(f\"Average Cost/Call: ${summary['avg_cost_per_call']:.4f}\")\n",
    "        print(\"\\nRecent Calls:\")\n",
    "        \n",
    "        for call in self.calls[-5:]:\n",
    "            print(f\"  {call['timestamp'][:19]} | \"\n",
    "                  f\"{call['model']:12} | \"\n",
    "                  f\"{call['total_tokens']:5} tokens | \"\n",
    "                  f\"${call['total_cost']:.4f}\")\n",
    "        \n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Example usage\n",
    "tracker = CostTracker()\n",
    "\n",
    "# Make several API calls\n",
    "test_prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain neural networks briefly.\",\n",
    "    \"What are transformers in AI?\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        input=prompt,\n",
    "        max_output_tokens=100\n",
    "    )\n",
    "    \n",
    "    call_data = tracker.track_response(\n",
    "        response,\n",
    "        model=\"gpt-5-mini\",\n",
    "        metadata={\"prompt_index\": i}\n",
    "    )\n",
    "    \n",
    "    print(f\"Call {i+1}: {call_data['total_tokens']} tokens, ${call_data['total_cost']:.4f}\")\n",
    "\n",
    "# Print final report\n",
    "tracker.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Context Window Manager\n",
    "\n",
    "Long conversations can exceed context windows. This pattern manages conversation history intelligently, using strategies like truncation, summarization, and sliding windows - essential for agents handling extended interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruncationStrategy(Enum):\n",
    "    \"\"\"Strategies for handling context overflow.\"\"\"\n",
    "    SLIDING_WINDOW = \"sliding_window\"  # Keep most recent messages\n",
    "    SUMMARIZE = \"summarize\"  # Summarize old messages\n",
    "    HYBRID = \"hybrid\"  # Combine strategies\n",
    "\n",
    "class ContextWindowManager:\n",
    "    \"\"\"Manage conversation context to stay within token limits.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        client: OpenAI,\n",
    "        max_tokens: int = 4000,\n",
    "        strategy: TruncationStrategy = TruncationStrategy.SLIDING_WINDOW,\n",
    "        buffer_tokens: int = 500\n",
    "    ):\n",
    "        self.client = client\n",
    "        self.max_tokens = max_tokens\n",
    "        self.strategy = strategy\n",
    "        self.buffer_tokens = buffer_tokens\n",
    "        self.messages: List[Dict] = []\n",
    "        self.summary: Optional[str] = None\n",
    "    \n",
    "    def add_message(self, role: str, content: str):\n",
    "        \"\"\"Add a message to context.\"\"\"\n",
    "        self.messages.append({\"role\": role, \"content\": content})\n",
    "    \n",
    "    def estimate_tokens(self, messages: List[Dict]) -> int:\n",
    "        \"\"\"Estimate token count for messages.\"\"\"\n",
    "        # Rough estimation: ~4 chars per token\n",
    "        total_chars = sum(len(msg['content']) for msg in messages)\n",
    "        return total_chars // 4\n",
    "    \n",
    "    def apply_sliding_window(self, messages: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Keep most recent messages that fit in context.\"\"\"\n",
    "        result = []\n",
    "        current_tokens = 0\n",
    "        \n",
    "        # Process messages in reverse (most recent first)\n",
    "        for msg in reversed(messages):\n",
    "            msg_tokens = self.estimate_tokens([msg])\n",
    "            if current_tokens + msg_tokens <= self.max_tokens - self.buffer_tokens:\n",
    "                result.insert(0, msg)\n",
    "                current_tokens += msg_tokens\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def summarize_messages(self, messages: List[Dict]) -> str:\n",
    "        \"\"\"Summarize old messages to save tokens.\"\"\"\n",
    "        # Combine messages into text\n",
    "        text = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in messages])\n",
    "        \n",
    "        # Create summary using the API\n",
    "        response = self.client.responses.create(\n",
    "            model=\"gpt-5-mini\",\n",
    "            input=f\"Summarize this conversation concisely:\\n\\n{text}\",\n",
    "            max_output_tokens=200\n",
    "        )\n",
    "        \n",
    "        return response.output_text\n",
    "    \n",
    "    def get_managed_context(self) -> List[Dict]:\n",
    "        \"\"\"Get context managed according to strategy.\"\"\"\n",
    "        current_tokens = self.estimate_tokens(self.messages)\n",
    "        \n",
    "        # If within limits, return as-is\n",
    "        if current_tokens <= self.max_tokens - self.buffer_tokens:\n",
    "            return self.messages\n",
    "        \n",
    "        print(f\"âš ï¸  Context overflow: {current_tokens} tokens. Applying {self.strategy.value}...\")\n",
    "        \n",
    "        if self.strategy == TruncationStrategy.SLIDING_WINDOW:\n",
    "            return self.apply_sliding_window(self.messages)\n",
    "        \n",
    "        elif self.strategy == TruncationStrategy.SUMMARIZE:\n",
    "            # Keep recent messages, summarize old ones\n",
    "            recent_count = 4  # Keep last 4 messages\n",
    "            old_messages = self.messages[:-recent_count]\n",
    "            recent_messages = self.messages[-recent_count:]\n",
    "            \n",
    "            if old_messages:\n",
    "                summary = self.summarize_messages(old_messages)\n",
    "                return [\n",
    "                    {\"role\": \"system\", \"content\": f\"Previous conversation summary: {summary}\"},\n",
    "                    *recent_messages\n",
    "                ]\n",
    "            return recent_messages\n",
    "        \n",
    "        elif self.strategy == TruncationStrategy.HYBRID:\n",
    "            # Summarize very old, keep middle, retain recent\n",
    "            if len(self.messages) > 10:\n",
    "                very_old = self.messages[:len(self.messages)//2]\n",
    "                recent = self.messages[len(self.messages)//2:]\n",
    "                \n",
    "                summary = self.summarize_messages(very_old)\n",
    "                managed = [\n",
    "                    {\"role\": \"system\", \"content\": f\"Summary: {summary}\"},\n",
    "                    *self.apply_sliding_window(recent)\n",
    "                ]\n",
    "                return managed\n",
    "            \n",
    "            return self.apply_sliding_window(self.messages)\n",
    "        \n",
    "        return self.messages\n",
    "\n",
    "# Example usage\n",
    "manager = ContextWindowManager(\n",
    "    client,\n",
    "    max_tokens=2000,\n",
    "    strategy=TruncationStrategy.SLIDING_WINDOW\n",
    ")\n",
    "\n",
    "# Simulate a long conversation\n",
    "conversation_turns = [\n",
    "    (\"user\", \"What is Python?\"),\n",
    "    (\"assistant\", \"Python is a high-level programming language...\"),\n",
    "    (\"user\", \"What are its main features?\"),\n",
    "    (\"assistant\", \"Python's main features include...\"),\n",
    "    (\"user\", \"How do I install it?\"),\n",
    "    (\"assistant\", \"You can install Python by...\"),\n",
    "    (\"user\", \"What are virtual environments?\"),\n",
    "    (\"assistant\", \"Virtual environments are isolated Python installations...\"),\n",
    "]\n",
    "\n",
    "for role, content in conversation_turns:\n",
    "    # Add some filler to simulate longer messages\n",
    "    filled_content = content + (\" Additional context.\" * 50)\n",
    "    manager.add_message(role, filled_content)\n",
    "\n",
    "# Get managed context\n",
    "managed = manager.get_managed_context()\n",
    "print(f\"\\nOriginal messages: {len(manager.messages)}\")\n",
    "print(f\"Managed messages: {len(managed)}\")\n",
    "print(f\"Estimated tokens: {manager.estimate_tokens(managed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Response Caching Pattern\n",
    "\n",
    "Caching responses can significantly reduce costs and latency for repeated queries. This pattern implements a simple but effective caching layer - useful for agents that handle similar queries or have frequently accessed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from datetime import timedelta\n",
    "\n",
    "@dataclass\n",
    "class CachedResponse:\n",
    "    \"\"\"Cached response with metadata.\"\"\"\n",
    "    text: str\n",
    "    response_id: str\n",
    "    timestamp: datetime\n",
    "    tokens: int\n",
    "    model: str\n",
    "\n",
    "class ResponseCache:\n",
    "    \"\"\"Cache responses to reduce API calls and costs.\"\"\"\n",
    "    \n",
    "    def __init__(self, ttl: timedelta = timedelta(hours=24)):\n",
    "        self.cache: Dict[str, CachedResponse] = {}\n",
    "        self.ttl = ttl\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "        self.tokens_saved = 0\n",
    "    \n",
    "    def _generate_key(self, model: str, input_text: str, **kwargs) -> str:\n",
    "        \"\"\"Generate cache key from parameters.\"\"\"\n",
    "        # Include important parameters that affect output\n",
    "        key_params = {\n",
    "            \"model\": model,\n",
    "            \"input\": input_text,\n",
    "            \"temperature\": kwargs.get(\"temperature\", 1.0),\n",
    "            \"max_tokens\": kwargs.get(\"max_output_tokens\"),\n",
    "            \"instructions\": kwargs.get(\"instructions\", \"\")\n",
    "        }\n",
    "        \n",
    "        # Create hash\n",
    "        key_str = json.dumps(key_params, sort_keys=True)\n",
    "        return hashlib.sha256(key_str.encode()).hexdigest()[:16]\n",
    "    \n",
    "    def _is_expired(self, cached: CachedResponse) -> bool:\n",
    "        \"\"\"Check if cached response is expired.\"\"\"\n",
    "        age = datetime.now() - cached.timestamp\n",
    "        return age > self.ttl\n",
    "    \n",
    "    def get(self, model: str, input_text: str, **kwargs) -> Optional[CachedResponse]:\n",
    "        \"\"\"Get cached response if available.\"\"\"\n",
    "        key = self._generate_key(model, input_text, **kwargs)\n",
    "        \n",
    "        if key in self.cache:\n",
    "            cached = self.cache[key]\n",
    "            \n",
    "            if not self._is_expired(cached):\n",
    "                self.hits += 1\n",
    "                self.tokens_saved += cached.tokens\n",
    "                return cached\n",
    "            else:\n",
    "                # Remove expired entry\n",
    "                del self.cache[key]\n",
    "        \n",
    "        self.misses += 1\n",
    "        return None\n",
    "    \n",
    "    def set(self, response, model: str, input_text: str, **kwargs):\n",
    "        \"\"\"Cache a response.\"\"\"\n",
    "        key = self._generate_key(model, input_text, **kwargs)\n",
    "        \n",
    "        self.cache[key] = CachedResponse(\n",
    "            text=response.output_text,\n",
    "            response_id=response.id,\n",
    "            timestamp=datetime.now(),\n",
    "            tokens=response.usage.total_tokens,\n",
    "            model=model\n",
    "        )\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get cache statistics.\"\"\"\n",
    "        total_requests = self.hits + self.misses\n",
    "        hit_rate = (self.hits / total_requests * 100) if total_requests > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"hits\": self.hits,\n",
    "            \"misses\": self.misses,\n",
    "            \"hit_rate\": hit_rate,\n",
    "            \"tokens_saved\": self.tokens_saved,\n",
    "            \"cached_entries\": len(self.cache)\n",
    "        }\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear all cached responses.\"\"\"\n",
    "        self.cache.clear()\n",
    "\n",
    "class CachedClient:\n",
    "    \"\"\"OpenAI client wrapper with caching.\"\"\"\n",
    "    \n",
    "    def __init__(self, client: OpenAI, cache: ResponseCache):\n",
    "        self.client = client\n",
    "        self.cache = cache\n",
    "    \n",
    "    def create_response(self, model: str, input: str, **kwargs):\n",
    "        \"\"\"Create response with caching.\"\"\"\n",
    "        # Check cache first\n",
    "        cached = self.cache.get(model, input, **kwargs)\n",
    "        \n",
    "        if cached:\n",
    "            print(f\"âœ“ Cache hit! Saved {cached.tokens} tokens\")\n",
    "            return cached\n",
    "        \n",
    "        # Cache miss - call API\n",
    "        print(f\"âŠ— Cache miss - calling API\")\n",
    "        response = self.client.responses.create(\n",
    "            model=model,\n",
    "            input=input,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # Cache the response\n",
    "        self.cache.set(response, model, input, **kwargs)\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Example usage\n",
    "cache = ResponseCache(ttl=timedelta(minutes=30))\n",
    "cached_client = CachedClient(client, cache)\n",
    "\n",
    "# Make the same query multiple times\n",
    "query = \"What is the capital of France?\"\n",
    "\n",
    "print(\"Request 1:\")\n",
    "response1 = cached_client.create_response(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=query\n",
    ")\n",
    "print(f\"Response: {response1.text if isinstance(response1, CachedResponse) else response1.output_text}\\n\")\n",
    "\n",
    "print(\"Request 2 (same query):\")\n",
    "response2 = cached_client.create_response(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=query\n",
    ")\n",
    "print(f\"Response: {response2.text if isinstance(response2, CachedResponse) else response2.output_text}\\n\")\n",
    "\n",
    "print(\"Request 3 (same query):\")\n",
    "response3 = cached_client.create_response(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=query\n",
    ")\n",
    "\n",
    "# Show cache statistics\n",
    "stats = cache.get_stats()\n",
    "print(f\"\\nðŸ“Š Cache Statistics:\")\n",
    "print(f\"   Hit rate: {stats['hit_rate']:.1f}%\")\n",
    "print(f\"   Hits: {stats['hits']}, Misses: {stats['misses']}\")\n",
    "print(f\"   Tokens saved: {stats['tokens_saved']}\")\n",
    "print(f\"   Cached entries: {stats['cached_entries']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook presented 10 essential patterns for building production-ready agentic applications:\n",
    "\n",
    "1. **Streaming Response Handler** - Real-time feedback with callbacks\n",
    "2. **Retry Logic** - Reliable API calls with exponential backoff\n",
    "3. **Token Budget Manager** - Cost control and usage tracking\n",
    "4. **Multi-Turn Conversation Builder** - Managing conversation state\n",
    "5. **Error Handling Wrapper** - Robust error management\n",
    "6. **Function Calling Pattern** - Tool integration for agents\n",
    "7. **Structured Output Parser** - Validated, structured responses\n",
    "8. **Cost Tracking Utility** - Budget monitoring and reporting\n",
    "9. **Context Window Manager** - Smart history management\n",
    "10. **Response Caching** - Performance and cost optimization\n",
    "\n",
    "Each pattern addresses real production challenges and can be adapted for your specific use case. Combine these patterns to build sophisticated agents that are reliable, cost-effective, and maintainable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
